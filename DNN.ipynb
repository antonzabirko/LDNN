{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import sigmoid, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.56500168, -0.25971907, -0.24950269, -1.66116313],\n       [ 0.01429791, -0.47222707,  0.13103123,  0.91423348],\n       [ 0.48889291,  0.67402905,  1.24669556,  0.44188237],\n       [-1.39882227,  1.31102149, -0.83196813,  0.38986131],\n       [ 0.42116466, -0.15580217,  0.99854994, -1.04392225]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# forward with X and parameters\n",
    "# compute cost with AL and Y\n",
    "# backprop with AL and cached values\n",
    "# adjust parameters\n",
    "\n",
    "def backward(AL, Y, caches):\n",
    "    gradients = {} # gradients\n",
    "    L = len(caches) # number of layers\n",
    "    m = AL.shape[1] # number of data samples; training sample count\n",
    "    # pop the Lth cache\n",
    "    (linear_cache, activation_cache) = caches[0]\n",
    "    # first do sigmoid backward\n",
    "    dZL = backward_activation(\"sigmoid\")\n",
    "    # then iterate linear ReLU backwards, backwards\n",
    "    # do the Lth backwards\n",
    "    dA_prev = backward_activity(dZL, linear_cache)\n",
    "    # now we add dA, dW, and db to the gradients\n",
    "    gradients[\"dA\" + str(L - 1)], gradients[\"dW\" + str(L)], gradients[\"db\" + str(L)] \\\n",
    "        = backward_activity(dZL, linear_cache)\n",
    "    for l in reversed(range(1, L - 1)):\n",
    "        # pop the caches\n",
    "        (linear_cache, activation_cache) = caches[l]\n",
    "        # relu backwards\n",
    "        dZ = backward_activation(dA_prev, \"ReLU\")\n",
    "        # linear backwards\n",
    "        dW, dA, db = backward_activity(dZ, linear_cache)\n",
    "        dA_prev = dA\n",
    "        # we need to add dW and db to our gradients\n",
    "        gradients[\"dA\" + str(l - 1)], gradients[\"dW\" + str(l)], gradients[\"db\" + str(l)] \\\n",
    "            = backward_activity(dZ, linear_cache)\n",
    "    return gradients\n",
    "    \n",
    "\n",
    "def backward_activity(dZ, linear_cache):\n",
    "    # here we do Wx + b derived\n",
    "    (W, b, A) = linear_cache\n",
    "    dW = (1 / m) * np.dot(dZ, A.T)\n",
    "    db = (1 / m) * np.sum(dZ)\n",
    "    dA = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dW, dA, db\n",
    "\n",
    "\n",
    "def backward_activation(dA, activation_cache, activation=\"ReLU\"):\n",
    "    if activation == \"ReLU\":\n",
    "        dZ = sigmoid_backwards(dA, activation_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = relu_backwards(dA, activation_cache)\n",
    "        \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def calc_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = - (1 / m) * np.sum(np.dot(Y, np.log(AL.T) + np.dot((1 - Y), np.log(1 - AL.T))))\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    return cost\n",
    "\n",
    "\n",
    "def forward(X, parameters):\n",
    "    # Z = forward_activity(A, W, b)\n",
    "    # A = forward_activation(Z, activation)\n",
    "    # We need to return an array of caches so we can use the saved values for backprop\n",
    "    # We return the the final activity, AL, A[L], of our forward propagation step so that\n",
    "    #   it can be used as the input for back-propagation\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    caches = []\n",
    "        \n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        W = parameters[\"W\" + str(l)]\n",
    "        b = parameters[\"b\" + str(l)]\n",
    "        Z, linear_cache = forward_activity(A_prev, W, b)\n",
    "        A, activation_cache = forward_activation(Z, W, b, \"ReLU\")\n",
    "        cache = (linear_cache, activation_cache)\n",
    "        caches.append(cache)\n",
    "    W = parameters[\"W\" + str(L)]\n",
    "    b = parameters[\"b\" + str(L)]\n",
    "    ZL, linear_cache = forward_activity(A, W, b)\n",
    "    AL, activation_cache = forward_activation(ZL, W, b, \"sigmoid\")\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches\n",
    "\n",
    "\n",
    "def forward_activation(Z, W, b, activation=\"ReLU\"):\n",
    "    if activation == \"ReLU\":\n",
    "        A, activation_cache = relu(Z)\n",
    "    elif activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    return A, activation_cache\n",
    "\n",
    "\n",
    "def forward_activity(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    linear_cache = (A, W, b)\n",
    "    \n",
    "    assert Z.shape == (W.shape[0], A.shape[1]), \"Not getting a proper dimension for Z (activity)\"\n",
    "    return Z, linear_cache\n",
    "\n",
    "\n",
    "def initialize_parameters(layers):\n",
    "    \"\"\"\n",
    "    Initializes the weights and biases for every layer of the neural network.\n",
    "    \n",
    "    :param layers: a Python array where the index is the layer and the value is the number of units in layer index\n",
    "    :return parameters: Python dictionary containing \"W1\", \"b1\", ..., \"WL\", \"bL\"\n",
    "                         - Wl: weights for l with shape: (layers[l], layers[l - 1])\n",
    "                         - bl: biases  for l with shape: (layers[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(layers)\n",
    "    parameters = {}\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layers[l], layers[l - 1])\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layers[l], 1))\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def train(X, Y, layers, learning_rate, num_iterations=2500):\n",
    "    parameters = initialize_parameters(layers)\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = forward(X, parameters)\n",
    "        cost = calc_cost(AL, Y)\n",
    "        # print out current cost\n",
    "        print(\"Cost at iteration \" + str(i) + \": \" + str(cost))\n",
    "        # do backprop here\n",
    "        dW, dA, db = backward(AL, caches)\n",
    "        # apply gradient descent and learning\n",
    "        parameters = update_parameters(dW, db, parameters, learning_rate)\n",
    "\n",
    "\n",
    "def update_parameters(dW, db, parameters, learning_rate=0.001):\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * dW\n",
    "        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * db\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "stem_cell": {
   "cell_type": "raw",
   "source": "",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
